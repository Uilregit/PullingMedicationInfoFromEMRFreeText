{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0   bed eight michael wu forty eight year under dr...\n",
      "1   vera abbott bed four under dr liu come in with...\n",
      "2   cindy benedictos bed under dr smtih come in fo...\n",
      "4   ramanathan nisha pillai bed year old under dr ...\n",
      "11  rebecca birks bed seven yr old under dalianis ...\n",
      "   Unnamed: 0  N=1  N=2  N=3  N=4  N=5  N=6  N=7   N=8   N=9\n",
      "0          mg  262  407  492  614  763  851  953  1068  1155\n",
      "1          on  152  268  315  383  447  505  553   595   625\n",
      "2       daily  113  187  256  318  379  419  482   527   570\n",
      "3         day   58  101  124  170  217  245  282   323   352\n",
      "4  medication   46   61   84  112  139  161  185   204   222\n",
      "5         for   46   91  127  158  189  220  263   303   341\n",
      "6          qd   42   62   82   98  120  129  146   162   173\n",
      "7         prn   40   56   73   92  111  124  136   152   159\n",
      "8     regular   40   51   60   65   72   73   83    88    91\n",
      "9         bid   39   68   92  118  145  166  191   210   225\n",
      "         0    1\n",
      "0       of  767\n",
      "1       in  684\n",
      "2     with  615\n",
      "3  patient  521\n",
      "4      for  504\n",
      "---\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"TokensTest.csv\", index_col = 0)\n",
    "keyTokens = pd.read_csv(\"NeighbourhoodCountsTrain.csv\")\n",
    "antiTokens = pd.read_csv(\"AntiNeighbourCountTrain.csv\", header = None)\n",
    "#antiTokens = antiTokens.iloc[:,0].values\n",
    "\n",
    "print (data.head())\n",
    "print (keyTokens.head(10))\n",
    "print (antiTokens.head())\n",
    "\n",
    "for i in range(len(keyTokens[\"N=1\"])):\n",
    "    if (keyTokens[\"N=1\"][0:i].sum()/float(keyTokens[\"N=1\"].sum()) >=0.7):\n",
    "        print (\"---\")\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting to see if the words are nouns, adverbs, adjectives\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
    "# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
    "tag_map = {\n",
    "        'JJ':wn.ADJ, # adjective (yellow)                         \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    try:\n",
    "        if tag_map[tag] != None:\n",
    "            return tag_map[tag]\n",
    "    except:\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def GetNoun (tokens):\n",
    "    tokens = nltk.pos_tag(tokens)\n",
    "    output = set()\n",
    "    for i in tokens:\n",
    "        if get_wordnet_pos(i[1]) == wn.NOUN:\n",
    "            output.add(i[0])\n",
    "    if len(output) >= 0:\n",
    "        return list(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMeds (neighbourhood = 1, numberOfWords = 1):\n",
    "    meds = [[]]\n",
    "    counter = 0\n",
    "    for i in range(len(data)):\n",
    "        tokenized = data.iloc[i,0].split(\" \")\n",
    "        nouns = []\n",
    "        for keyToken in keyTokens.iloc[:73,0]:\n",
    "            try:\n",
    "                while keyToken in tokenized:\n",
    "                    keyLocation = tokenized.index(keyToken)\n",
    "                    tokenWindow = tokenized[max(keyLocation-neighbourhood,0):keyLocation+neighbourhood+1]\n",
    "                    #tokenWindow = tokenized[keyLocation:keyLocation+neighbourhood+1]\n",
    "                    tokenWindow.remove(keyToken)\n",
    "                    for token in tokenWindow:\n",
    "                        if token in antiTokens:\n",
    "                            counter += 1\n",
    "                            break\n",
    "                    nouns += GetNoun(tokenWindow)\n",
    "                    tokenized.pop(keyLocation)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        meds += [[\",\".join(list(set(nouns)))]]  \n",
    "    print (\"tokens removed:\",counter)\n",
    "    return meds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens removed: 0\n",
      "13.400266647338867\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "meds = GetMeds(3)\n",
    "endTime = time.time()\n",
    "print (endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsDF = pd.DataFrame(meds)\n",
    "medsDF = medsDF.iloc[1:]\n",
    "medsDF.index = data.index\n",
    "medsDF.to_csv(\"tokenminer_results.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vera abbott bed four under dr liu come in with chest pain with history of stroke previous chest pain asthma cataract glaucoma almost blind need assistance nitros with effect still under monitoring'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"TextToken\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vera', 'NN'), ('abbott', 'NN'), ('bed', 'VBD'), ('four', 'CD'), ('under', 'IN')]\n",
      "{'vera'}\n",
      "{'vera', 'abbott'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'set' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-5646e519dd51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vera'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'abbott'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'four'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'under'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mGetNoun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vera'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'abbott'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'four'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'under'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-8cae73c7d399>\u001b[0m in \u001b[0;36mGetNoun\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'set' and 'int'"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(['vera', 'abbott', 'bed', 'four', 'under'])\n",
    "print (tags)\n",
    "print (GetNoun(['vera', 'abbott', 'bed', 'four', 'under']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if \"of\" in antiTokens.iloc[:,0].values:\n",
    "    print (\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
